{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-26T12:41:22.128686Z","iopub.execute_input":"2022-04-26T12:41:22.129077Z","iopub.status.idle":"2022-04-26T12:41:22.1601Z","shell.execute_reply.started":"2022-04-26T12:41:22.128967Z","shell.execute_reply":"2022-04-26T12:41:22.159462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# stanford dog breed 데이터 세트 다운로드 \n!wget http://vision.stanford.edu/aditya86/ImageNetDogs/images.tar\n# 현재 디렉토리인 /kaggle/working에 바로 압축 해제 \n!ls; tar -xvf images.tar","metadata":{"execution":{"iopub.status.busy":"2022-04-26T12:41:28.276002Z","iopub.execute_input":"2022-04-26T12:41:28.276262Z","iopub.status.idle":"2022-04-26T12:43:31.390851Z","shell.execute_reply.started":"2022-04-26T12:41:28.276234Z","shell.execute_reply":"2022-04-26T12:43:31.38995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential , Model\nfrom tensorflow.keras.layers import Input, Dense , Conv2D , Dropout , Flatten , Activation, MaxPooling2D , GlobalAveragePooling2D\nfrom tensorflow.keras.optimizers import Adam , RMSprop \nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau , EarlyStopping , ModelCheckpoint , LearningRateScheduler\n\nfrom tensorflow.keras.applications import Xception, ResNet50V2, EfficientNetB0, EfficientNetB1\nfrom tensorflow.keras.applications import MobileNet\nimport tensorflow as tf\n\n# dog breed 종류는 120가지\n\ndef create_model(model_type='xception', in_shape=(224, 224, 3), n_classes=120):\n    input_tensor = Input(shape=in_shape)\n    if model_type == 'resnet50v2':\n        base_model = tf.keras.applications.ResNet50V2(include_top=False, weights='imagenet', input_tensor=input_tensor)\n    elif model_type == 'xception':\n        base_model = tf.keras.applications.Xception(include_top=False, weights='imagenet', input_tensor=input_tensor)\n    elif model_type == 'efficientnetb0':\n        base_model = tf.keras.applications.EfficientNetB0(include_top=False, weights='imagenet', input_tensor=input_tensor)\n    elif model_type == 'efficientnetb1':\n        base_model = tf.keras.applications.EfficientNetB1(include_top=False, weights='imagenet', input_tensor=input_tensor)\n    \n    x = base_model.output\n    x = GlobalAveragePooling2D()(x)\n    x = Dense(1024,activation='relu')(x)\n    x = Dropout(0.5)(x)\n    preds = Dense(units=n_classes,activation='softmax')(x)\n    \n    model = Model(inputs = input_tensor, outputs= preds)\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2022-04-26T12:43:31.394631Z","iopub.execute_input":"2022-04-26T12:43:31.394891Z","iopub.status.idle":"2022-04-26T12:43:37.006168Z","shell.execute_reply.started":"2022-04-26T12:43:31.394857Z","shell.execute_reply":"2022-04-26T12:43:37.005337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\n\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.utils import Sequence\nimport sklearn \nimport cv2\n\nimport albumentations as A\n\nIMAGE_DIR = '/kaggle/working/Images'\n\ndef make_dogbreed_dataframe(image_dir=IMAGE_DIR):\n    paths=[]\n    label_gubuns=[]\n    for dirname,_,filenames in os.walk(image_dir):\n        for filename in filenames:\n            if '.jpg' in filename:\n                file_path = dirname+'/'+filename\n                paths.append(file_path)\n                start_pos = file_path.find('/',20)\n                end_pos = file_path.rfind('/')\n                imsi_breed = file_path[start_pos+1:end_pos]\n                breed = imsi_breed[imsi_breed.find('-')+1:]\n                label_gubuns.append(breed)\n    \n    data_df = pd.DataFrame({'path':paths, 'label':label_gubuns})\n    return data_df\n\ndef get_train_valid(train_df, valid_size=0.2, random_state=2021):\n    train_path = train_df['path'].values\n    train_label = pd.get_dummies(train_df['label']).values\n    \n    tr_path, val_path, tr_label, val_label = train_test_split(train_path, train_label, test_size=valid_size, random_state= random_state)\n    return tr_path, val_path, tr_label, val_label\n\nclass Breed_Dataset(Sequence):\n    def __init__(self, image_filenames, labels, image_size=224, batch_size=64, augmentor=None, shuffle=False, pre_func=None):\n        self.image_filenames = image_filenames\n        self.labels = labels\n        self.image_size = image_size\n        self.batch_size = batch_size\n        self.augmentor = augmentor\n        self.pre_func = pre_func\n        # train data의 경우 \n        self.shuffle = shuffle\n        if self.shuffle:\n            self.on_epoch_end()\n    \n    def __len__(self):\n        return int(np.ceil(len(self.labels)/self.batch_size))\n    \n    def __getitem__(self, index):\n        image_name_batch = self.image_filenames[index*self.batch_size:(index+1)*self.batch_size]\n        if self.labels is not None:\n            label_batch = self.labels[index*self.batch_size:(index+1)*self.batch_size]\n        image_batch = np.zeros((image_name_batch.shape[0],self.image_size,self.image_size,3))\n        for image_index in range(image_name_batch.shape[0]):\n            image = cv2.cvtColor(cv2.imread(image_name_batch[image_index]),cv2.COLOR_BGR2RGB)\n            if self.augmentor is not None:\n                image = self.augmentor(image=image)['image']\n            image = cv2.resize(image, (self.image_size, self.image_size))\n            if self.pre_func is not None:\n                image = self.pre_func(image)\n            image_batch[image_index] = image\n        return image_batch, label_batch\n    \n    def on_epoch_end(self):\n        if self.shuffle:\n            self.image_filenames, self.labels = sklearn.utils.shuffle(self.image_filenames, self.labels)\n        else:\n            pass\n        \n        \naugmentor_light = A.Compose([\n    A.HorizontalFlip(p=0.5)\n])\n\nIMAGE_DIR = '/kaggle/working/Images'\n\ndata_df = make_dogbreed_dataframe(image_dir=IMAGE_DIR)\ntrain_df, test_df = train_test_split(data_df, test_size=0.4, stratify=data_df['label'], random_state=2021)\nprint(train_df.shape, test_df.shape)\n\n\n            ","metadata":{"execution":{"iopub.status.busy":"2022-04-26T13:06:10.547343Z","iopub.execute_input":"2022-04-26T13:06:10.547671Z","iopub.status.idle":"2022-04-26T13:06:10.686405Z","shell.execute_reply.started":"2022-04-26T13:06:10.547637Z","shell.execute_reply":"2022-04-26T13:06:10.685477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"N_EPOCHS = 30\nBATCH_SIZE = 64\nIMAGE_SIZE = 224\n\ndef train_model(model_type, train_df, initial_lr=0.001, augmentor=None, input_pre_func=None):\n    tr_path, val_path, tr_label, val_label = get_train_valid(train_df, valid_size=0.2, random_state=2021)\n    \n    tr_ds = Breed_Dataset(tr_path, tr_label, image_size=IMAGE_SIZE,batch_size=BATCH_SIZE, augmentor = augmentor, shuffle=True, pre_func=input_pre_func)\n    val_ds = Breed_Dataset(val_path, val_label, image_size=IMAGE_SIZE, batch_size=BATCH_SIZE, \n                          augmentor=None, shuffle=False, pre_func=input_pre_func)\n    \n    model = create_model(model_type = model_type)\n    model.compile(optimizer = Adam(initial_lr), loss = 'categorical_crossentropy', metrics=['accuracy'])\n    \n    rlr_cb = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, mode='min', verbose=1)\n    # 10번 iteration내에 validation loss가 향상되지 않으면 더 이상 학습하지 않고 종료\n    ely_cb = EarlyStopping(monitor='val_loss', patience=10, mode='min', verbose=1)\n    \n    history = model.fit(tr_ds, epochs=N_EPOCHS, steps_per_epoch=tr_path.shape[0]//BATCH_SIZE, \n                   validation_data=val_ds, validation_steps=val_path.shape[0]//BATCH_SIZE,\n                   callbacks=([rlr_cb, ely_cb]), verbose=1)\n    \n    return model, history","metadata":{"execution":{"iopub.status.busy":"2022-04-26T13:06:10.932667Z","iopub.execute_input":"2022-04-26T13:06:10.933231Z","iopub.status.idle":"2022-04-26T13:06:10.94539Z","shell.execute_reply.started":"2022-04-26T13:06:10.933193Z","shell.execute_reply":"2022-04-26T13:06:10.944478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport cv2\nimport seaborn as sns\n%matplotlib inline \n\ndef show_grid_images(image_path_list, augmentor=None, ncols=4, title=None):\n    figure, axs = plt.subplots(figsize=(22,4),nrows=1, ncols=ncols)\n    for i in range(ncols):\n        image = cv2.cvtColor(cv2.imread(image_path_list[i]),cv2.COLOR_BGR2RGB)\n        image = cv2.resize(image,(224,224))\n        if augmentor is not None:\n            image = augmentor(image=image)['image']\n        axs[i].imshow(image)\n        axs[i].axis('off')\n        axs[i].set_title(title)\n        \n                \nbreed_image_list_01 = data_df[data_df['label']=='Siberian_husky']['path'].iloc[:6].tolist()\nbreed_image_list_02 = data_df[data_df['label']=='Eskimo_dog']['path'].iloc[:6].tolist()\n\nshow_grid_images(breed_image_list_01, ncols=6, title='Siberian_husky')\nshow_grid_images(breed_image_list_02, ncols=6, title='Eskimo_dog')","metadata":{"execution":{"iopub.status.busy":"2022-04-26T13:06:11.271Z","iopub.execute_input":"2022-04-26T13:06:11.27153Z","iopub.status.idle":"2022-04-26T13:06:12.653623Z","shell.execute_reply.started":"2022-04-26T13:06:11.271474Z","shell.execute_reply":"2022-04-26T13:06:12.652747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"augmentor_heavy_01 = A.Compose([\n    A.HorizontalFlip(p=0.5),\n    A.ShiftScaleRotate(p=0.2),\n    A.CenterCrop(height=90, width=90, p=0.2),\n    A.RandomBrightnessContrast(brightness_limit=(-0.2,0.2), contrast_limit=(-0.2,0.2),p=0.2),\n    A.ColorJitter(p=0.2),\n    A.OneOf([\n        A.CoarseDropout(p=1, max_holes=26),\n        A.CLAHE(p=1),\n        A.Blur(blur_limit=(10,15),p=1)\n    ],p=0.3)\n    \n])","metadata":{"execution":{"iopub.status.busy":"2022-04-26T13:06:12.655262Z","iopub.execute_input":"2022-04-26T13:06:12.655689Z","iopub.status.idle":"2022-04-26T13:06:12.671975Z","shell.execute_reply.started":"2022-04-26T13:06:12.655636Z","shell.execute_reply":"2022-04-26T13:06:12.670554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"breed_image_list_01 = data_df[data_df['label']=='Staffordshire_bullterrier']['path'].iloc[:6].tolist()       \nshow_grid_images(breed_image_list_01, augmentor=None, ncols=6, title='orignal Staffordshire_bullterrier')\nshow_grid_images(breed_image_list_01, augmentor=augmentor_heavy_01, ncols=6, title='augmented')","metadata":{"execution":{"iopub.status.busy":"2022-04-26T13:06:12.673251Z","iopub.execute_input":"2022-04-26T13:06:12.673581Z","iopub.status.idle":"2022-04-26T13:06:14.072305Z","shell.execute_reply.started":"2022-04-26T13:06:12.67354Z","shell.execute_reply":"2022-04-26T13:06:14.071465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.applications.efficientnet import preprocess_input as eff_preprocess_input\n\neffb0_model_aug01, effb0_hisory_aug01 = train_model(model_type='efficientnetb0',train_df=train_df, initial_lr = 0.0001, augmentor = augmentor_heavy_01, input_pre_func = eff_preprocess_input)","metadata":{"execution":{"iopub.status.busy":"2022-04-26T13:06:14.681962Z","iopub.execute_input":"2022-04-26T13:06:14.682597Z","iopub.status.idle":"2022-04-26T13:47:19.62937Z","shell.execute_reply.started":"2022-04-26T13:06:14.682557Z","shell.execute_reply":"2022-04-26T13:47:19.624476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"breed_image_list_01 = data_df[data_df['label']=='Staffordshire_bullterrier']['path'].iloc[:6].tolist()       \nshow_grid_images(breed_image_list_01, augmentor=None, ncols=6, title='orignal Staffordshire_bullterrier')\nshow_grid_images(breed_image_list_01, augmentor=augmentor_heavy_01, ncols=6, title='augmented')\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict_result = effb0_model_aug01.predict(test_ds, steps=int(np.ceil(len(test_label)/BATCH_SIZE)))\npredict_class = np.argmax(predic_result,axis=1)\n\ntest_df['effb0_aug01_pred_class']=predict_class\n\nprint(test_df[test_df['gt_class']!=test_df['effb0_aug01_pred_class']]['label'].value_counts())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"augmentor_heavy_02 = A.Compose([\n    A.HorizontalFlip(p=0.5),\n    A.ShiftScaleRotate(p=0.3),\n    A.RandomBrightnessContrast(brightness_limit=(-0.2, 0.2), contrast_limit=(-0.2, 0.2), p=0.3),\n    A.ColorJitter(p=0.3),\n    A.OneOf(\n        [A.CoarseDropout(p=0.3, max_holes=26), \n         A.CLAHE(p=0.3),\n         A.Blur(blur_limit=(10, 15), p=0.3)\n        ], p=0.3)\n])\n\nfrom tensorflow.keras.applications.efficientnet import preprocess_input as eff_preprocess_input\n\neffb0_model_aug02, effb0_history_aug02 = train_model(model_type='efficientnetb0', train_df=train_df, initial_lr=0.0001,\n                                               augmentor=augmentor_heavy_02, input_pre_func=eff_preprocess_input)\n\ntest_path = test_df['path'].values\ntest_label = pd.get_dummies(test_df['label']).values\n\n\ntest_ds = Breed_Dataset(test_path, test_label, image_size=IMAGE_SIZE, batch_size=BATCH_SIZE, \n                        augmentor=None, shuffle=False, pre_func=eff_preprocess_input)\n\neffb0_model_aug02.evaluate(test_ds)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def lrfn(epcoh):\n    LR_START=1e-5\n    LR_MAX=1e-4\n    LR_RAMPUP_EPOCHS=2\n    LR_SUSTAIN_EPOCHS=2\n    LR_STEP_DECAY.75\n    \n    def calc_fn(epoch):\n        if epoch < LR_RAMPUP_EPOCHS:\n            lr = (LR_MAX-LR_START)/LR_RAMPUP_EPOCHS*epoch + LR_START\n        elif epoch < LR_RAMPUP_EPOCHS+LR_SUSTAIN_EPOCHS:\n            lr = lr\n        else :\n            lr = LR_MAX * LR_STEP_DECAY ** ((epoch-LR_RAMPUP_EPOCHS-LR_SUSTAIN_EPOCHS)//2)\n        return lr\n    return calc_fn(epoch)\n\n","metadata":{"execution":{"iopub.status.busy":"2022-04-26T13:47:19.630408Z","iopub.status.idle":"2022-04-26T13:47:19.630933Z","shell.execute_reply.started":"2022-04-26T13:47:19.630697Z","shell.execute_reply":"2022-04-26T13:47:19.630722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"N_EPOCHS = 30\nBATCH_SIZE = 64\nIMAGE_SIZE = 224\n\ndef train_model_with_aug_lr(model_type, train_df, initial_lr=0.001, augmentor=None, callbacks_list=None, input_pre_func=None):\n    \n    tr_path, val_path, tr_label, val_label = get_train_valid(train_df, valid_size=0.2, random_state=2021)\n    \n    tr_ds = Breed_Dataset(tr_path, tr_label, image_size=IMAGE_SIZE, batch_size=BATCH_SIZE, \n                          augmentor=augmentor, shuffle=True, pre_func=input_pre_func)\n    val_ds = Breed_Dataset(val_path, val_label, image_size=IMAGE_SIZE, batch_size=BATCH_SIZE, \n                          augmentor=None, shuffle=False, pre_func=input_pre_func)\n\n    # model_type인자로 들어온 모델 생성. optimizer Adam적용. \n    print('#######', model_type, ' 생성 및 학습 수행 ########')\n    model = create_model(model_type=model_type)\n    model.compile(optimizer=Adam(lr=initial_lr), loss='categorical_crossentropy', metrics=['accuracy'])\n    # learning rate scheduler와 early stopping 을 함수 인자로 입력 받음. \n    history = model.fit(tr_ds, epochs=N_EPOCHS, steps_per_epoch=tr_path.shape[0]//BATCH_SIZE, \n                   validation_data=val_ds, validation_steps=val_path.shape[0]//BATCH_SIZE,\n                   callbacks=(callbacks_list), verbose=1)\n    \n    return model, history","metadata":{"execution":{"iopub.status.busy":"2022-04-26T13:47:19.640586Z","iopub.status.idle":"2022-04-26T13:47:19.643073Z","shell.execute_reply.started":"2022-04-26T13:47:19.642818Z","shell.execute_reply":"2022-04-26T13:47:19.642846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.applications.efficientnet import preprocess_input as eff_preprocess_input\nimport tensorflow as tf\n\nlr_cb = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=1)\nely_cb = tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=10, mode='min',berbose=1)\ncallbacks_list = [lr_cb, ely_cb]\n\naugmentor_heavy_01 = A.Compose([\n    A.HorizontalFlip(p=0.5),\n    A.ShiftScaleRotate(p=0.2),\n    A.CenterCrop(height=90, width=90, p=0.2),\n    A.RandomBrightnessContrast(brightness_limit=(-0.2, 0.2), contrast_limit=(-0.2, 0.2), p=0.2),\n    A.ColorJitter(p=0.2),\n    A.OneOf(\n        [A.CoarseDropout(p=1, max_holes=26), \n         A.CLAHE(p=1),\n         A.Blur(blur_limit=(10, 15), p=1)\n        ], p=0.3)\n])\n\naugmentor_heavy_02 = A.Compose([\n    A.HorizontalFlip(p=0.5),\n    A.ShiftScaleRotate(p=0.3),\n    A.RandomBrightnessContrast(brightness_limit=(-0.2, 0.2), contrast_limit=(-0.2, 0.2), p=0.3),\n    A.ColorJitter(p=0.3),\n    A.OneOf(\n        [A.CoarseDropout(p=0.3, max_holes=26), \n         A.CLAHE(p=0.3),\n         A.Blur(blur_limit=(10, 15), p=0.3)\n        ], p=0.3)\n])\n\n# augmentor_heavy_01을 ramp up and step decay 적용. \neffb0_model_lr01, effb0_history_lr01 = train_model_with_aug_lr(model_type='efficientnetb0', train_df=train_df, initial_lr=0.0001, \n                                               augmentor=augmentor_heavy_01, callbacks_list=callbacks_list, input_pre_func=eff_preprocess_input)\n\ntest_path = test_df['path'].values\ntest_label = pd.get_dummies(test_df['label']).values\n\ntest_ds = Breed_Dataset(test_path, test_label, image_size=IMAGE_SIZE, batch_size=BATCH_SIZE, \n                        augmentor=None, shuffle=False, pre_func=eff_preprocess_input)\n\neffb0_model_lr01.evaluate(test_ds)\n\nfrom tensorflow.keras.applications.efficientnet import preprocess_input as eff_preprocess_input\nimport tensorflow as tf\n​\n# augmentor_heavy_02를 ramp up and step decay 적용. \neffb0_model_lr02, effb0_history_lr02 = train_model_with_aug_lr(model_type='efficientnetb0', train_df=train_df, initial_lr=0.0001, \n                                               augmentor=augmentor_heavy_02, callbacks_list=callbacks_list, input_pre_func=eff_preprocess_input)\n\ntest_ds = Breed_Dataset(test_path, test_label, image_size=IMAGE_SIZE, batch_size=BATCH_SIZE, \n                        augmentor=None, shuffle=False, pre_func=eff_preprocess_input)\n\neffb0_model_lr02.evaluate(test_ds)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_imsi = create_model(model_type='efficientnetb0')\nmodel_imsi.summary()","metadata":{"execution":{"iopub.status.busy":"2022-04-26T13:47:32.193337Z","iopub.execute_input":"2022-04-26T13:47:32.193977Z","iopub.status.idle":"2022-04-26T13:47:33.881888Z","shell.execute_reply.started":"2022-04-26T13:47:32.193939Z","shell.execute_reply":"2022-04-26T13:47:33.88109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras import layers\nimport tensorflow as tf\n\nN_EPOCHS = 30\nBATCH_SIZE = 64\nIMAGE_SIZE = 224\n\ndef train_model_with_ft(model_type, train_df, initial_lr=0.0001, augmentor=None, callbacks_list=None, input_pre_func=None):\n    \n    tr_path, val_path, tr_label, val_label = get_train_valid(train_df, valid_size=0.2, random_state=2021)\n\n    tr_ds = Breed_Dataset(tr_path, tr_label, image_size=IMAGE_SIZE, batch_size=BATCH_SIZE, \n                        augmentor=augmentor, shuffle=True, pre_func=input_pre_func)\n    val_ds = Breed_Dataset(val_path, val_label, image_size=IMAGE_SIZE, batch_size=BATCH_SIZE, \n                        augmentor=None, shuffle=False, pre_func=input_pre_func)\n\n    model = create_model(model_type = model_type)\n    \n    for layer in model.layers[:-4]:\n        layer.trainable = False\n        \n    model.compile(optimizer=Adam(initial_lr),loss='categorical_crossentropy',metrics=['accuracy'])\n    ely_cb = EarlyStopping(monitor='val_loss', patience=10, mode='min', verbose=1)\n    lr_scheduler = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=1)\n    \n    history = model.fit(tr_ds, epochs=15, steps_per_epoch=int(np.ceil(tr_path.shape[0]/BATCH_SIZE)), \n                  validation_data=val_ds, validation_steps=int(np.ceil(val_path.shape[0]/BATCH_SIZE)),\n                  callbacks=(callbacks_list), verbose=1)\n    \n    for layer in model.layers:\n        if not isinstance(layer, layers.BatchNormalization):\n            layer.trainable=True\n    \n    history = model.fit(tr_ds, epochs=25, steps_per_epoch=int(np.ceil(tr_path.shape[0]/BATCH_SIZE)), \n                  validation_data=val_ds, validation_steps=int(np.ceil(val_path.shape[0]/BATCH_SIZE)),\n                  callbacks=(callbacks_list), verbose=1)\n\n    return model, history\n    ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.applications.efficientnet import preprocess_input as eff_preprocess_input\nimport tensorflow as tf\n\n# learning rate scheduler에 적용할 함수 선언. 내포 함수를 사용. \ndef lrfn(epoch):\n    # 내포 함수인 calc_fn()에서 사용되는 파라미터. \n    LR_START = 1e-5\n    LR_MAX = 1e-4\n    LR_RAMPUP_EPOCHS = 2\n    LR_SUSTAIN_EPOCHS = 1\n    LR_STEP_DECAY = 0.75\n    \n    def calc_fn(epoch):\n        if epoch < LR_RAMPUP_EPOCHS:\n            lr = (LR_MAX - LR_START) / LR_RAMPUP_EPOCHS * epoch + LR_START\n        elif epoch < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:\n            lr = LR_MAX\n        else:\n            lr = LR_MAX * LR_STEP_DECAY**((epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS)//2)\n        return lr\n    \n    # 반드시 내포 함수인 calc_fn(epoch)를 호출해야함. \n    return calc_fn(epoch)\n\n# Learning Rate Scheduler(Ramp up and step down decay) 와 Early Stopping callback 생성. \nlr_cb = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=1)\nely_cb = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, mode='min', verbose=1)\n# train_model에 인자로 넣을 Callback 객체의 리스트 생성. \ncallbacks_list = [lr_cb, ely_cb]\n\n\naugmentor_light_01 = A.Compose([\n    A.HorizontalFlip(p=0.5),\n])\n\naugmentor_light_02 = A.Compose([\n    A.HorizontalFlip(p=0.3),\n    A.ShiftScaleRotate(scale_limit=(0.7, 0.9), p=0.2, rotate_limit=30),\n    A.RandomBrightnessContrast(brightness_limit=(-0.2, 0.2), contrast_limit=(-0.2, 0.2), p=0.2),\n    A.ColorJitter(p=0.2)\n])\n\naugmentor_heavy_01 = A.Compose([\n    A.HorizontalFlip(p=0.5),\n    A.ShiftScaleRotate(p=0.2),\n    A.CenterCrop(height=90, width=90, p=0.2),\n    A.RandomBrightnessContrast(brightness_limit=(-0.2, 0.2), contrast_limit=(-0.2, 0.2), p=0.2),\n    A.ColorJitter(p=0.2),\n    A.OneOf(\n        [A.CoarseDropout(p=1, max_holes=26), \n         A.CLAHE(p=1),\n         A.Blur(blur_limit=(10, 15), p=1)\n        ], p=0.3)\n])\n\naugmentor_heavy_02 = A.Compose([\n    A.HorizontalFlip(p=0.5),\n    A.ShiftScaleRotate(p=0.3),\n    A.RandomBrightnessContrast(brightness_limit=(-0.2, 0.2), contrast_limit=(-0.2, 0.2), p=0.3),\n    A.ColorJitter(p=0.3),\n    A.OneOf(\n        [A.CoarseDropout(p=0.3, max_holes=26), \n         A.CLAHE(p=0.3),\n         A.Blur(blur_limit=(10, 15), p=0.3)\n        ], p=0.3)\n])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"effb0_model_ft01, effb0_history_ft01 = train_model_with_ft(model_type='efficientnetb0', train_df=train_df, initial_lr=0.0001, \n                                            augmentor=augmentor_heavy_01, callbacks_list=callbacks_list, input_pre_func=eff_preprocess_input)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_path = test_df['path'].values\ntest_label = pd.get_dummies(test_df['label']).values\n\ntest_ds = Breed_Dataset(test_path, test_label, image_size=IMAGE_SIZE, batch_size=BATCH_SIZE, \n                        augmentor=None, shuffle=False, pre_func=eff_preprocess_input)\n\neffb0_model_ft01.evaluate(test_ds)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"effb0_model_ft02, effb0_history_ft02 = train_model_with_ft(model_type='efficientnetb0', train_df=train_df, initial_lr=0.0001, \n                                            augmentor=augmentor_light_02, callbacks_list=callbacks_list, input_pre_func=eff_preprocess_input)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_ds = Breed_Dataset(test_path, test_label, image_size=IMAGE_SIZE, batch_size=BATCH_SIZE, \n                        augmentor=None, shuffle=False, pre_func=eff_preprocess_input)\n\neffb0_model_ft02.evaluate(test_ds)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Config:\n    MODEL_TYPE = 'efficientnetb0'\n    IMAGE_SIZE = 224\n    BATCH_SIZE=64\n    N_EPOCHS=30\n    IS_FINE_TUNING = False\n    FIRST_EPOCHS=15\n    SECOND_EPOCHS=25\n    FIRST_CALLBACKS=None\n    SECOND_CALLBACKS=None\n    AUGMENTOR=None\n    PRE_FUNC=None\n    INITIAL_LR = 0.0001\n    DEBUG=False","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential , Model\nfrom tensorflow.keras.layers import Input, Dense , Conv2D , Dropout , Flatten , Activation, MaxPooling2D , GlobalAveragePooling2D\nfrom tensorflow.keras.optimizers import Adam , RMSprop \nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau , EarlyStopping , ModelCheckpoint , LearningRateScheduler\n\nfrom tensorflow.keras.applications import Xception, ResNet50V2, EfficientNetB0, EfficientNetB1, EfficientNetB2, EfficientNetB3\nfrom tensorflow.keras.applications import MobileNet\nimport tensorflow as tf\n\n# dog breed 종류는 120가지\nN_CLASSES = 120\n\ndef create_model(model_type='xception', in_shape=(224, 224, 3), n_classes=120):\n    input_tensor = Input(shape=in_shape)\n\n    if model_type == 'resnet50v2':\n        base_model = tf.keras.applications.ResNet50V2(include_top=False, weights='imagenet', input_tensor=input_tensor)\n    elif model_type == 'xception':\n        base_model = tf.keras.applications.Xception(include_top=False, weights='imagenet', input_tensor=input_tensor)\n    elif model_type == 'efficientnetb0':\n        base_model = tf.keras.applications.EfficientNetB0(include_top=False, weights='imagenet', input_tensor=input_tensor)\n    elif model_type == 'efficientnetb1':\n        base_model = tf.keras.applications.EfficientNetB1(include_top=False, weights='imagenet', input_tensor=input_tensor)\n    elif model_type == 'efficientnetb2':\n        base_model = tf.keras.applications.EfficientNetB2(include_top=False, weights='imagenet', input_tensor=input_tensor)\n    elif model_type == 'efficientnetb3':\n        base_model = tf.keras.applications.EfficientNetB3(include_top=False, weights='imagenet', input_tensor=input_tensor)\n        \n    x = base_model.output  \n    x = GlobalAveragePooling2D()(x)\n    x = Dense(1024, activation='relu')(x)\n    x = Dropout(0.5)(x)    \n    preds = Dense(units=n_classes, activation='softmax')(x)\n    model = Model(inputs=input_tensor, outputs=preds)\n    \n    return model","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\n\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.utils import Sequence\nimport sklearn \nimport cv2\n\nimport albumentations as A\n\nIMAGE_DIR = '/kaggle/working/Images'\n\ndef make_dogbreed_dataframe(image_dir=IMAGE_DIR):\n    paths = []\n    label_gubuns = []\n    for dirname, _, filenames in os.walk(image_dir):\n        for filename in filenames:\n            # 이미지 파일이 아닌 파일도 해당 디렉토리에 있음.\n            if '.jpg' in filename:\n                # 파일의 절대 경로를 file_path 변수에 할당. \n                file_path = dirname+'/'+ filename\n                paths.append(file_path)\n                # 이미지 파일의 절대 경로에서 레이블명 생성을 위한 1차 추출. '/'로 분할하여 파일 바로 위 서브디렉토리 이름 가져옴.  \n                start_pos = file_path.find('/', 20)\n                end_pos = file_path.rfind('/')\n                imsi_breed = file_path[start_pos+1:end_pos]\n                # 1차 추출된 데이터를 기반으로 '-' 이후 데이터가 레이블 값임. \n                breed = imsi_breed[imsi_breed.find('-')+1:]\n                #print(start_pos, end_pos, imsi_breed)\n                label_gubuns.append(breed)\n\n    data_df = pd.DataFrame({'path':paths, 'label':label_gubuns})\n    return data_df\n\n# 학습과 검증 데이터용 numpy array 분리. \ndef get_train_valid(train_df, valid_size=0.2, random_state=2021):\n    train_path = train_df['path'].values\n    train_label = pd.get_dummies(train_df['label']).values\n    \n    tr_path, val_path, tr_label, val_label = train_test_split(train_path, train_label, test_size=valid_size, random_state=random_state)\n    print('tr_path shape:', tr_path.shape, 'tr_label shape:', tr_label.shape, 'val_path shape:', val_path.shape, 'val_label shape:', val_label.shape)\n    return tr_path, val_path, tr_label, val_label\n\n\n# 입력 인자 image_filenames, labels는 모두 numpy array로 들어옴. \nclass Breed_Dataset(Sequence):\n    def __init__(self, image_filenames, labels, image_size=224, batch_size=64, \n                 augmentor=None, shuffle=False, pre_func=None):\n        '''\n        파라미터 설명\n        image_filenames: opencv로 image를 로드할 파일의 절대 경로들\n        labels: 해당 image의 label들\n        batch_size: __getitem__(self, index) 호출 시 마다 가져올 데이터 batch 건수\n        augmentor: albumentations 객체\n        shuffle: 학습 데이터의 경우 epoch 종료시마다 데이터를 섞을지 여부\n        '''\n        # 객체 생성 인자로 들어온 값을 객체 내부 변수로 할당. \n        self.image_filenames = image_filenames\n        self.labels = labels\n        self.image_size = image_size\n        self.batch_size = batch_size\n        self.augmentor = augmentor\n        self.pre_func = pre_func\n        # train data의 경우 \n        self.shuffle = shuffle\n        if self.shuffle:\n            # 객체 생성시에 한번 데이터를 섞음. \n            #self.on_epoch_end()\n            pass\n    \n    # Sequence를 상속받은 Dataset은 batch_size 단위로 입력된 데이터를 처리함. \n    # __len__()은 전체 데이터 건수가 주어졌을 때 batch_size단위로 몇번 데이터를 반환하는지 나타남\n    def __len__(self):\n        # batch_size단위로 데이터를 몇번 가져와야하는지 계산하기 위해 전체 데이터 건수를 batch_size로 나누되, 정수로 정확히 나눠지지 않을 경우 1회를 더한다. \n        return int(np.ceil(len(self.labels) / self.batch_size))\n    \n    # batch_size 단위로 image_array, label_array 데이터를 가져와서 변환한 뒤 다시 반환함\n    # 인자로 몇번째 batch 인지를 나타내는 index를 입력하면 해당 순서에 해당하는 batch_size 만큼의 데이타를 가공하여 반환\n    # batch_size 갯수만큼 변환된 image_array와 label_array 반환. \n    def __getitem__(self, index):\n        # index는 몇번째 batch인지를 나타냄. \n        # batch_size만큼 순차적으로 데이터를 가져오려면 array에서 index*self.batch_size:(index+1)*self.batch_size 만큼의 연속 데이터를 가져오면 됨\n        image_name_batch = self.image_filenames[index*self.batch_size:(index+1)*self.batch_size]\n        if self.labels is not None:\n            label_batch = self.labels[index*self.batch_size:(index+1)*self.batch_size]\n        \n        # 만일 객체 생성 인자로 albumentation으로 만든 augmentor가 주어진다면 아래와 같이 augmentor를 이용하여 image 변환\n        # albumentations은 개별 image만 변환할 수 있으므로 batch_size만큼 할당된 image_name_batch를 한 건씩 iteration하면서 변환 수행. \n        # image_batch 배열은 float32 로 설정. \n        image_batch = np.zeros((image_name_batch.shape[0], self.image_size, self.image_size, 3), dtype='float32')\n        \n        # batch_size에 담긴 건수만큼 iteration 하면서 opencv image load -> image augmentation 변환(augmentor가 not None일 경우)-> image_batch에 담음. \n        for image_index in range(image_name_batch.shape[0]):\n            image = cv2.cvtColor(cv2.imread(image_name_batch[image_index]), cv2.COLOR_BGR2RGB)\n            if self.augmentor is not None:\n                image = self.augmentor(image=image)['image']\n            #crop 시 잘린 이미지가 원본 이미지와 다르게 되므로 augmentation 적용 후 resize() 적용. \n            image = cv2.resize(image, (self.image_size, self.image_size))\n            # 만일 preprocessing_input이 pre_func인자로 들어오면 이를 이용하여 scaling 적용. \n            if self.pre_func is not None:\n                image = self.pre_func(image)\n                \n            image_batch[image_index] = image\n        \n        return image_batch, label_batch\n    \n    # epoch가 한번 수행이 완료 될 때마다 모델의 fit()에서 호출됨. \n    def on_epoch_end(self):\n        if(self.shuffle):\n            #print('epoch end')\n            # 전체 image 파일의 위치와 label를 쌍을 맞춰서 섞어준다. scikt learn의 utils.shuffle에서 해당 기능 제공\n            self.image_filenames, self.labels = sklearn.utils.shuffle(self.image_filenames, self.labels)\n        else:\n            pass\n        \n        \naugmentor_light = A.Compose([\n    A.HorizontalFlip(p=0.5),\n])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_model(train_df, config=Config):\n    tr_path, val_path, tr_label, val_label = get_train_valid(train_df, valid_size=0.2, random_state=2021)\n    \n    tr_ds = Breed_Dataset(tr_path, tr_label, image_size=config.IMAGE_SIZE, batch_size=config.BATCH_SIZE,augmentor = config.AUGMENTOR,shuffle=True, pre_func=config.PRE_FUNC)\n    val_ds = Breed_Dataset(val_path, val_label, image_size=config.IMAGE_SIZE, batch_size=config.BATCH_SIZE, augmentor=None, shuffle=False, pre_func=config.PRE_FUNC)\n    \n    if config.DEBUG:\n        tr_image_batch=next(iter(tr_ds))[0]\n        val_image_batch = next(iter(val_ds))[0]\n        print(tr_image_batch.shape, val_image_batch.shape)\n        print(tr_image_batch[0], val_image_batch[0])\n        \n    model = create_model(model_type=config.MODEL_TYPE, in_shape=(config.IMAGE_SIZE,config.IMAGE_SIZE,3),n_classes=120)\n    model.compile(optimizer=Adam(config.INITIAL_LR),loss='categorical_crossentropy', metrics=['accuracy'])\n    \n    if config.IS_FINE_TUNING:\n        for layer in model.layers[:-4]:\n            layer.trainable=False\n        \n        history = model.fit(tr_ds, epochs = config.FIRST_EPOCHS,validation_data=val_ds,callbacks=(config.FIRST_CALLBACKS),verbose=1)\n        \n        for layer in model.layers:\n            if not isinstance(layer, layers.BatchNormalization):\n                layer.trainable=True\n        \n        history = model.fit(tr_ds, epochs = config.SECOND_EPOCHS, validation_data=val_ds, callbacks=(config.SECOND_CALLBACKS),verbose=1)\n        \n    else :\n        history = model.fit(tr_ds, epochs=config.N_EPOCHS, steps_per_epoch=int(np.ceil(tr_path.shape[0]/config.BATCH_SIZE)), \n                       validation_data=val_ds, validation_steps=int(np.ceil(val_path.shape[0]/config.BATCH_SIZE)),\n                       callbacks=(config.FIRST_CALLBACKS), verbose=1)\n        \n    return model, history","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.applications.efficientnet import preprocess_input as eff_preprocess_input\nimport tensorflow as tf\n​\n# learning rate scheduler에 적용할 함수 선언. \ndef lrfn_01(epoch):\n    LR_START = 1e-5\n    LR_MAX = 1e-4\n    LR_RAMPUP_EPOCHS = 2\n    LR_SUSTAIN_EPOCHS = 1\n    LR_STEP_DECAY = 0.75\n    \n    def calc_fn(epoch):\n        if epoch < LR_RAMPUP_EPOCHS:\n            lr = (LR_MAX - LR_START) / LR_RAMPUP_EPOCHS * epoch + LR_START\n        elif epoch < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:\n            lr = LR_MAX\n        else:\n            lr = LR_MAX * LR_STEP_DECAY**((epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS)//2)\n        return lr\n    \n    return calc_fn(epoch)\n​\ndef lrfn_02(epoch):\n    LR_START = 1e-6\n    LR_MAX = 2e-5\n    LR_RAMPUP_EPOCHS = 2\n    LR_SUSTAIN_EPOCHS = 1\n    LR_STEP_DECAY = 0.75\n    \n    def calc_fn(epoch):\n        if epoch < LR_RAMPUP_EPOCHS:\n            lr = (LR_MAX - LR_START) / LR_RAMPUP_EPOCHS * epoch + LR_START\n        elif epoch < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:\n            lr = LR_MAX\n        else:\n            lr = LR_MAX * LR_STEP_DECAY**((epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS)//2)\n        return lr\n    \n    return calc_fn(epoch)\n\nlr01_cb = tf.keras.callbacks.LearningRateScheduler(lrfn_01, verbose=1)\nlr02_cb = tf.keras.callbacks.LearningRateScheduler(lrfn_02, verbose=1)\nely_cb = tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=10,mode='min',verbose=1)\n\naugmentor_light_02 = A.Compose([\n    A.HorizontalFlip(p=0.3),\n    A.ShiftScaleRotate(scale_limit=(0.7, 0.9), p=0.2, rotate_limit=30),\n    A.RandomBrightnessContrast(brightness_limit=(-0.2, 0.2), contrast_limit=(-0.2, 0.2), p=0.2),\n    A.ColorJitter(p=0.2)\n])\n\nclass Config:\n    MODEL_TYPE = 'efficientnetb1'\n    IMAGE_SIZE=240\n    BATCH_SIZE=64\n    N_EPOCHS=30\n    IS_FINE_TUNING = True\n    FIRST_EPOCHS=15\n    SECOND_EPOCHS=15\n    FIRST_CALLBACKS=[lr01_cb,ely_cb]\n    SECOND_CALLBACKS=[lr02_cb, ely_cb]\n    AUGMENTOR = augmentor_light_02\n    PRE_FUNC = eff_preprocess_input\n    INITIAL_LR=0.0001\n    DEBUG=False","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eff1_model, history = train_model(train_df, config=Config)\n\ndef evaluate_model(model, test_df, config=Config):\n    test_path = test_df['path'].values\n    test_label = pd.get_dummies(test_df['label']).values\n    test_ds = Breed_Dataset(test_path, test_label, image_size=config.IMAGE_SIZE, batch_size=config.BATCH_SIZE, \n                        augmentor=None, shuffle=False, pre_func=config.PRE_FUNC)\n\n    evaluation_result = model.evaluate(test_ds)\n    print('evaluation_result:', evaluation_result)\n    \n    return model, evaluation_result\n\nmodel, evaluation_result = evaluate_model(model=eff1_model, test_df=test_df, config=Config)","metadata":{},"execution_count":null,"outputs":[]}]}